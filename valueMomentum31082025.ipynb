{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHKF33R8zqHjURRilzMFrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anvy87/AlgoTrading/blob/master/valueMomentum31082025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "icpsFL16x89l",
        "outputId": "21a8465c-d43c-4a94-fa19-1e32b10763be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting beautifulsoup4==4.11.2\n",
            "  Downloading beautifulsoup4-4.11.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4==4.11.2) (2.7)\n",
            "Downloading beautifulsoup4-4.11.2-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: beautifulsoup4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.5\n",
            "    Uninstalling beautifulsoup4-4.13.5:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.5\n",
            "Successfully installed beautifulsoup4-4.11.2\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4==4.11.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dJd0JWdWx95E",
        "outputId": "5de96756-9be2-400e-b38e-3cba754513ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DSekhivZyI7x",
        "outputId": "380f4567-ec87-4e6a-bbca-4ec963dc123a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import logging"
      ],
      "metadata": {
        "id": "t3brjcDPyJip"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_logging():\n",
        "    # Configure Application Logging\n",
        "    log_filepath = \"ToolsLog.log\"\n",
        "    format = \"%(asctime)s: - %(levelname)s - %(message)s\"\n",
        "    logging.basicConfig(\n",
        "            filename=log_filepath,\n",
        "            format=format,\n",
        "            level=logging.INFO,\n",
        "            datefmt=\"%Y-%m-%d %H:%M:%S\")"
      ],
      "metadata": {
        "id": "mUJFaYFCyOOX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_screener_data(link):\n",
        "    cache_index = None\n",
        "    data = pd.DataFrame()\n",
        "    current_page = 1\n",
        "    page_limit = 25\n",
        "    while current_page < page_limit:\n",
        "        if current_page == 1:\n",
        "            url=link\n",
        "        else:\n",
        "            url = f'{link}?page={current_page}'\n",
        "        all_tables = pd.read_html(url, flavor='bs4')\n",
        "        combined_df = pd.concat(all_tables)\n",
        "        combined_df = combined_df.drop(\n",
        "            combined_df[combined_df['S.No.'].isnull()].index)\n",
        "        # print(combined_df)\n",
        "        # if cache_index == combined_df.iloc[-2]['S.No.']:\n",
        "        if len(combined_df.index) < 26:\n",
        "            data = pd.concat([data, combined_df], axis=0)\n",
        "            break\n",
        "        # cache_index = combined_df.iloc[-2]['S.No.']\n",
        "        # print(cache_index)\n",
        "        data = pd.concat([data, combined_df], axis=0)\n",
        "        current_page += 1\n",
        "        time.sleep(1)\n",
        "    data = data.iloc[0:].drop(data[data['S.No.'] == 'S.No.'].index)\n",
        "    return data"
      ],
      "metadata": {
        "id": "j0IqKWmyyRWb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "UXeWFQPTyWdl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configure_logging()\n",
        "logging.info(\"Tools : Trending Value Screen extract commenced\")"
      ],
      "metadata": {
        "id": "M6sh3eJvyZ9_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch PE/PB/Dividend Yield Value Ratio\n",
        "pbv_link = 'https://www.screener.in/screens/2112737/trendvalue_pricebookvalue/'\n",
        "pbv_df = fetch_screener_data(pbv_link)\n",
        "pbv_df = pbv_df[['Name','P/E', 'Div Yld  %', 'CMP / BV']]\n",
        "pbv_df['P/E'] = pd.to_numeric(pbv_df['P/E'], errors='coerce')\n",
        "pbv_df['Div Yld  %'] = pd.to_numeric(pbv_df['Div Yld  %'], errors='coerce')\n",
        "pbv_df['CMP / BV'] = pd.to_numeric(pbv_df['CMP / BV'], errors='coerce')\n",
        "pbv_df['P/E'] = pbv_df['P/E'].fillna(100000)\n",
        "pbv_df['CMP / BV'] = pbv_df['CMP / BV'].fillna(100000)\n",
        "pbv_df['Div Yld  %'] = pbv_df['Div Yld  %'].fillna(0)\n",
        "# pbv_df.to_excel('D:/pbv.xlsx', index=False)\n",
        "pbv_df.dropna(inplace=True)\n",
        "pbv_df = pbv_df[pbv_df['P/E'] > 0]\n",
        "pbv_df = pbv_df[pbv_df['Div Yld  %'] > 0]\n",
        "merged_df = pbv_df"
      ],
      "metadata": {
        "id": "c2V0bDluycoH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Price to Free Cash Flow\n",
        "cashflow_link = 'https://www.screener.in/screens/2112756/trendvalue_cashflow/'\n",
        "cf_df = fetch_screener_data(cashflow_link)\n",
        "cf_df = cf_df[['Name','CMP / OCF']]\n",
        "cf_df['CMP / OCF'] = pd.to_numeric(cf_df['CMP / OCF'], errors='coerce')\n",
        "cf_df['CMP / OCF'] = cf_df['CMP / OCF'].fillna(100000)\n",
        "# cf_df.to_excel('D:/cashflow.xlsx', index=False)\n",
        "cf_df.dropna(inplace=True)\n",
        "cf_df = cf_df[cf_df['CMP / OCF'] > 0]\n",
        "merged_df = pd.merge(merged_df, cf_df, on='Name', how='inner')\n",
        "# print(\"Free Cash Flow data extraction complete\")"
      ],
      "metadata": {
        "id": "7MBH0ENvyulL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch EV to EBITDA\n",
        "ev_link = 'https://www.screener.in/screens/2112767/trendvalue_ev/'\n",
        "ev_df = fetch_screener_data(ev_link)\n",
        "ev_df = ev_df[['Name','EV / EBITDA']]\n",
        "ev_df['EV / EBITDA'] = pd.to_numeric(ev_df['EV / EBITDA'], errors='coerce')\n",
        "ev_df['EV / EBITDA'] = ev_df['EV / EBITDA'].fillna(100000)\n",
        "# ev_df.to_excel('D:/ev.xlsx', index=False)\n",
        "ev_df.dropna(inplace=True)\n",
        "ev_df = ev_df[ev_df['EV / EBITDA'] > 0]\n",
        "merged_df = pd.merge(merged_df, ev_df, on='Name', how='inner')\n",
        "# print(\"EV to EBDITA data extraction complete\")"
      ],
      "metadata": {
        "id": "pG1CdN1Xyy1_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Price to Sales ratio\n",
        "sales_link = 'https://www.screener.in/screens/2112772/trendvalue_pricesales/'\n",
        "sales_df = fetch_screener_data(sales_link)\n",
        "sales_df = sales_df[['Name','CMP / Sales']]\n",
        "sales_df['CMP / Sales'] = pd.to_numeric(sales_df['CMP / Sales'], errors='coerce')\n",
        "sales_df['CMP / Sales'] = sales_df['CMP / Sales'].fillna(100000)\n",
        "# sales_df.to_excel('D:/sales.xlsx', index=False)\n",
        "sales_df.dropna(inplace=True)\n",
        "sales_df = sales_df[sales_df['CMP / Sales'] > 0]\n",
        "# print(\"Price to Sales Ratio data extraction complete\")"
      ],
      "metadata": {
        "id": "OlCzFD_dy6qx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Last 6 months return (Momentum)\n",
        "momentum_link = 'https://www.screener.in/screens/2112742/trendvalue_momentum/'\n",
        "mo_df = fetch_screener_data(momentum_link)\n",
        "mo_df = mo_df[['Name','6mth return  %']]\n",
        "mo_df['6mth return  %'] = pd.to_numeric(mo_df['6mth return  %'], errors='coerce')\n",
        "mo_df['6mth return  %'] = mo_df['6mth return  %'].fillna(-100000)\n",
        "# mo_df.to_excel('D:/mo.xlsx', index=False)\n",
        "mo_df.dropna(inplace=True)\n",
        "mo_df = mo_df[mo_df['6mth return  %'] > 0]\n",
        "merged_df = pd.merge(merged_df, mo_df, on='Name', how='inner')\n",
        "# print(\"Momentum / 6 month Returns data extraction complete\")"
      ],
      "metadata": {
        "id": "BdcrZgCey-WS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Merged dataset\n",
        "merged_df = pd.merge(merged_df, sales_df, on='Name', how='inner')\n",
        "merged_df = merged_df.rename(columns={\n",
        "    'Name': 'Stock',\n",
        "    'P/E': 'PE',\n",
        "    'Div Yld  %': 'Div',\n",
        "    'CMP / BV': 'BV',\n",
        "    'CMP / OCF': 'Cashflow',\n",
        "    'EV / EBITDA': 'EV',\n",
        "    'CMP / Sales': 'Sales',\n",
        "    '6mth return  %': '6mo Return'\n",
        "})\n",
        "merged_df['PE'] = merged_df['PE'].map(lambda x: float(x))\n",
        "merged_df['Div'] = merged_df['Div'].map(lambda x: float(x))\n",
        "merged_df['BV'] = merged_df['BV'].map(lambda x: float(x))\n",
        "merged_df['6mo Return'] = merged_df['6mo Return'].map(lambda x: float(x))\n",
        "# merged_df['3mo Return'] = merged_df['3mo Return'].map(lambda x: float(x))\n",
        "merged_df['Cashflow'] = merged_df['Cashflow'].map(lambda x: float(x))\n",
        "merged_df['EV'] = merged_df['EV'].map(lambda x: float(x))\n",
        "merged_df['Sales'] = merged_df['Sales'].map(lambda x: float(x))"
      ],
      "metadata": {
        "id": "Syh6sPeAzBj4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Decile for PE\n",
        "merged_df['PE_Rank'] = merged_df['PE'].rank()\n",
        "merged_df['PE_Decile'] = pd.qcut(merged_df['PE_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "# Apply Decile for Div\n",
        "merged_df['Div_Rank'] = merged_df['Div'].rank(ascending=False)\n",
        "merged_df['Div_Decile'] = pd.qcut(merged_df['Div_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "# Apply Decile for BV\n",
        "merged_df['BV_Rank'] = merged_df['BV'].rank()\n",
        "merged_df['BV_Decile'] = pd.qcut(merged_df['BV_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "# Apply Decile for Cashflow\n",
        "merged_df['Cashflow_Rank'] = merged_df['Cashflow'].rank()\n",
        "merged_df['Cashflow_Decile'] = pd.qcut(merged_df['Cashflow_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "# Apply Decile for EV\n",
        "merged_df['EV_Rank'] = merged_df['EV'].rank()\n",
        "merged_df['EV_Decile'] = pd.qcut(merged_df['EV_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "# Apply Decile for Sales\n",
        "merged_df['Sales_Rank'] = merged_df['Sales'].rank()\n",
        "merged_df['Sales_Decile'] = pd.qcut(merged_df['Sales_Rank'], q=10, labels=False, duplicates='drop') + 1"
      ],
      "metadata": {
        "id": "tzayTRMEzIGY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidated_Rank column\n",
        "merged_df['Consolidated_Rank'] = merged_df['PE_Decile'] + merged_df['Div_Decile'] + merged_df['BV_Decile'] + merged_df['Cashflow_Decile'] + merged_df['EV_Decile'] + merged_df['Sales_Decile']"
      ],
      "metadata": {
        "id": "AsIOq2dWzNfL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain only stocks that has given a postive return in the last 6 months\n",
        "merged_df = merged_df[merged_df['6mo Return'] > 0]"
      ],
      "metadata": {
        "id": "-ZpGDYyOzZUb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgFF2EeK1Ycq",
        "outputId": "3e996bca-d7d4-464f-8820-aa7f2fc40c4f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decile on consolidated rank\n",
        "merged_df['Consolidated_Decile'] = pd.qcut(merged_df['Consolidated_Rank'], q=10, labels=False, duplicates='drop') + 1\n",
        "#merged_df = merged_df.sort_values(by=['Consolidated_Decile', '6mo Return'], ascending=[True, False])\n",
        "merged_df.to_excel('/content/drive/MyDrive/merged.xlsx', index=False)"
      ],
      "metadata": {
        "id": "Xrkf3Gnpzc6-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retain only rows in the first decile of Consolidated_Decile\n",
        "df_final = merged_df[merged_df['Consolidated_Decile'] == 1]\n",
        "df_final = df_final.sort_values(by=['6mo Return'], ascending=False)\n",
        "df_final = df_final.head(40)\n",
        "df_final.to_excel('/content/drive/MyDrive/final.xlsx', index=False)\n",
        "print(\"Final Dataset\")\n",
        "print(df_final)\n",
        "logging.info(\"Tools : Trending Value Screen extract completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJOIePTyz4fn",
        "outputId": "b15c0cd1-544f-401e-901f-b2af1939f478"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Dataset\n",
            "                Stock     PE   Div    BV  Cashflow     EV  6mo Return  Sales  \\\n",
            "62      Hero Motocorp  20.20  3.24  5.28     23.69  13.50       44.75   2.52   \n",
            "135     Indian Metals  12.99  2.32  1.98      7.95   8.80       39.63   1.83   \n",
            "158      AGI Greenpac  16.60  0.78  2.75     13.48   8.32       35.97   2.18   \n",
            "33       Sharda Motor  18.43  1.59  5.43     21.66  11.76       29.91   2.02   \n",
            "22                KSE   6.87  1.27  2.54      5.28   4.65       22.17   0.46   \n",
            "137      Welspun Corp  13.62  0.59  2.97     14.77   8.43       18.87   1.54   \n",
            "102     Nitta Gelatin   9.80  0.73  1.76      9.89   5.48       17.22   1.39   \n",
            "134  Expleo Solutions  16.14  4.82  2.53      8.97   7.95       15.49   1.56   \n",
            "122     Nucleus Soft.  16.18  1.23  3.34     18.02  10.71       15.22   3.18   \n",
            "66   Gulf Oil Lubric.  16.62  3.84  4.21     14.55   9.61       12.17   1.68   \n",
            "12   Accelya Solution  16.18  3.58  7.52     14.41  10.05       10.11   3.95   \n",
            "125  Automotive Axles  16.64  1.76  2.66     20.38  10.41        9.11   1.26   \n",
            "159  L G Balakrishnan  14.21  1.57  2.13     14.06   7.91        7.57   1.52   \n",
            "49       Natco Pharma   9.09  0.70  2.03      9.10   5.81        7.37   3.51   \n",
            "152      Ador Welding  36.68  2.19  3.13     11.57  17.13        6.21   1.44   \n",
            "42      Bombay Burmah  10.85  0.97  2.17      5.37   3.75        3.78   0.66   \n",
            "55       Alldigi Tech  22.25  3.09  5.70     12.46  10.01        3.62   2.64   \n",
            "114     Mahanagar Gas  11.40  2.40  2.10      9.04   6.94        3.13   1.69   \n",
            "\n",
            "     PE_Rank  PE_Decile  Div_Rank  Div_Decile  BV_Rank  BV_Decile  \\\n",
            "62      26.0          2       5.0           1     44.0          3   \n",
            "135      7.0          1      14.0           1      3.0          1   \n",
            "158     15.0          1      74.0           5     11.0          1   \n",
            "33      20.0          2      28.0           2     49.0          3   \n",
            "22       2.0          1      39.5           3      9.0          1   \n",
            "137      8.0          1      91.5           6     12.0          1   \n",
            "102      4.0          1      77.0           5      2.0          1   \n",
            "134     12.0          1       1.0           1      8.0          1   \n",
            "122     13.5          1      43.0           3     16.0          1   \n",
            "66      16.0          1       2.0           1     26.0          2   \n",
            "12      13.5          1       3.0           1     86.0          5   \n",
            "125     17.0          1      23.0           2     10.0          1   \n",
            "159      9.0          1      29.0           2      6.0          1   \n",
            "49       3.0          1      81.5           5      4.0          1   \n",
            "152     89.0          5      17.0           1     13.5          1   \n",
            "42       5.0          1      59.0           4      7.0          1   \n",
            "55      32.0          2       7.0           1     55.0          4   \n",
            "114      6.0          1      12.0           1      5.0          1   \n",
            "\n",
            "     Cashflow_Rank  Cashflow_Decile  EV_Rank  EV_Decile  Sales_Rank  \\\n",
            "62            28.0                2     29.0          2        34.0   \n",
            "135            3.0                1     11.0          1        21.0   \n",
            "158           10.0                1      9.0          1        28.0   \n",
            "33            24.0                2     23.0          2        23.0   \n",
            "22             1.0                1      3.0          1         1.0   \n",
            "137           16.0                1     10.0          1        11.0   \n",
            "102            7.0                1      4.0          1         5.0   \n",
            "134            4.0                1      8.0          1        12.0   \n",
            "122           19.0                2     16.0          1        50.0   \n",
            "66            13.0                1     12.0          1        16.5   \n",
            "12            12.0                1     14.0          1        65.0   \n",
            "125           22.0                2     15.0          1         4.0   \n",
            "159           11.0                1      7.0          1        10.0   \n",
            "49             6.0                1      5.0          1        56.0   \n",
            "152            8.0                1     49.0          3         6.5   \n",
            "42             2.0                1      2.0          1         2.0   \n",
            "55             9.0                1     13.0          1        36.0   \n",
            "114            5.0                1      6.0          1        18.0   \n",
            "\n",
            "     Sales_Decile  Consolidated_Rank  Consolidated_Decile  \n",
            "62              2                 12                    1  \n",
            "135             2                  7                    1  \n",
            "158             2                 11                    1  \n",
            "33              2                 13                    1  \n",
            "22              1                  8                    1  \n",
            "137             1                 11                    1  \n",
            "102             1                 10                    1  \n",
            "134             1                  6                    1  \n",
            "122             3                 11                    1  \n",
            "66              1                  7                    1  \n",
            "12              4                 13                    1  \n",
            "125             1                  8                    1  \n",
            "159             1                  7                    1  \n",
            "49              4                 13                    1  \n",
            "152             1                 12                    1  \n",
            "42              1                  9                    1  \n",
            "55              2                 11                    1  \n",
            "114             1                  6                    1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc98poGk2Jka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}